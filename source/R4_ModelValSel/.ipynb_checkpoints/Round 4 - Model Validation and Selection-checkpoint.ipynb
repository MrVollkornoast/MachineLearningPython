{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-97da75c1d52f0687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Machine Learning with Python - Model Validation and Selection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous rounds you have implemented supervised machine learning (ML) methods by combining particular choices for\n",
    "\n",
    "* data points, their features and labels, \n",
    "* a hypothesis space (of predictor functions) \n",
    "* and a loss function that measures the quality of a particular predictor function out of the hypothesis space. \n",
    "\n",
    "ML algorithms are optimization methods that try to find (or learn) the best predictor out of the hypothesis space by minimizing the average loss over some labeled data points (the training data). This predictor is then used to predict the labels of new data points.\n",
    "\n",
    "The previous rounds have focused on the problem of learning a good (accurate) predictor by minimizing the average loss incurred over some labeled (training) data points. Eventually, we are not primarily interested \n",
    "in how well the predictor is doing on **training data** but rather in how well it is doing on **new data which is different from the training data**. We want the learned predictor to **generalize** well to new data. \n",
    "\n",
    "The generalization of a predictor depends not only on the (amount of) training data but also on the hypothesis space (which we refer to as \"model\"). We can validate a particular choice for the hypothesis space (model) by a simple two-step process<\n",
    "* First, we train a predictor by minimizing the loss over the training data. The so-obtained predictor is the optimal predictor out of the hypothesis space in terms of training error. </li>\n",
    "* In a second (validation) step, we determine the loss (\"validation error\") of the so-obtained optimal predictor on some new data points, which is different from the training data.&nbsp;&nbsp;</li>\n",
    "\n",
    "The validation error allows to choose between different hypothesis spaces, i.e., to do <b>model selection</b>. Model selection is a simple but powerful tool to obtain ML method that generalize well to data which is different from the training data. \n",
    "\n",
    "Another approach to ensure good generalization is **regularization**. Roughly speaking, regularization aims at anticipating the increase in the loss, relative to the average loss on the training data, when the predictor is applied to new data. This anticipation can be based on symmetries or invariances (a rotated cat image is still a cat image) as well as on robustness requirements (zeroing few pixels of a cat image should still allow to classify it as a cat image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85d7ef6fc6ff2a13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We begin this round with an example that presents the problem of **overfitting** that is prevalent when fitting complex predictors to data. \n",
    "\n",
    "The code snippet below read in some data points $(x^{(i)},y^{(i)})$, for $i=1,2,\\ldots$, which are characterized by a scalar feature $x^{(i)}$ and a numeric label $y^{(i)}$. Note that we consider the datapoint in the last element of $y$ to be an outlier and remove this from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1aa568825aa58d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "from sklearn import datasets \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# load the toy dataset \"linnerud\" provide by the \"sklearn\" package\n",
    "linnerud = datasets.load_linnerud()\n",
    "# read in the exercise parameters (nr. of chinups ..) for each athlete\n",
    "Exercise = linnerud['data']\n",
    "# read in the physiological (weight ...) paramters for each athlete\n",
    "Physio = linnerud['target']\n",
    "\n",
    "x = Physio[:-1,0].reshape(-1,1) \n",
    "# convert Lbs to Kg\n",
    "x = x*0.453 \n",
    "# we use number of chinups as label and store them (for all athletes) in numpy array y\n",
    "y = Exercise[:-1,0] \n",
    "\n",
    "m_total = y.shape[0]  # Number of datapoints in the dataset\n",
    "print (\"Total number of labeled data points = \", m_total)\n",
    "\n",
    "# Plot a scatterplot of the dataset\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y,label=\"Labeled data points\")\n",
    "plt.ylabel('label ' + r'$y$')\n",
    "plt.xlabel('feature ' + r'$x$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fd784a728ac00b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we choose four data points as our **training set** in this example. The training set is the set on which we will fit the models. The rest of the data will be used to assess how well the model fitted on the training set predicts the values of data points that are not used in training. Conventionally, this set is called the **validation set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2601ad49c9617b02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose the datapoints in the 4 final elements as the training set\n",
    "x_train = x[m_total-4:]\n",
    "y_train = y[m_total-4:]\n",
    "\n",
    "# Let the rest be the validation set\n",
    "x_val = x[:m_total-4]\n",
    "y_val = y[:m_total-4]\n",
    "\n",
    "# Plot the points not in the training set and the points in the training set\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_val, y_val, label=\"labeled data points\")\n",
    "plt.scatter(x_train, y_train, s=400, marker=r'$\\times$', label=\"training set\")\n",
    "plt.ylabel('label ' + r'$y$')\n",
    "plt.xlabel('feature ' + r'$x$')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46af00bfd04c2706",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we will compare the fit of two polynomial predictors of different degree on the training set and analyze how well the optimal predictors generalize to the validation set.\n",
    "\n",
    "Using the four data points (marked by crosses in the above plot) in the training set, we learn (find) the best predictors out of the hypothesis spaces\n",
    "\n",
    "$$ \\mathcal{H}^{(4)} = \\{ h(x) = w_{0}+w_{1}x+w_{2}x^2+w_{3}x^3+w_{4}x^{4} \\mbox{ with tunable weights } w_{0},\\ldots,w_{4} \\in \\mathbb{R} \\}.$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\mathcal{H}^{(1)} = \\{ h(x) = w_{0}+w_{1}x \\mbox{ with tunable weights } w_{0},w_1 \\in \\mathbb{R} \\}.$$\n",
    "\n",
    "That is, we fit a fourth degree polynomial regression model and a linear regression model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7be1c2de8fb89d1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# choose best predictor out of the hypothesis space given by all \n",
    "# polynomials h(x) = w_0 + w_1*x ... + w_4*x^4 of maximum degree 4 \n",
    "poly = PolynomialFeatures(degree = 4) \n",
    "# transform scalar feature x to a feature vector [x^0 x^1 ... x^4]\n",
    "X_poly = poly.fit_transform(x_train) \n",
    "# we can now use linear regression using the transformed feature vectors \n",
    "poly_reg = LinearRegression() \n",
    "# compute optimal weights to minimize training error \n",
    "poly_reg.fit(X_poly, y_train) \n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_train, y_train)\n",
    "\n",
    "# Plot the resulting \"optimal\" predictor (having minimum training error) \n",
    "x_grid = np.linspace(69, 93, num=100)\n",
    "x_grid_2 = np.linspace(69, 113, num=100)\n",
    "X_poly = poly.fit_transform(x_grid.reshape(-1,1))\n",
    "\n",
    "# Plot the dataset and predictor functions\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_val, y_val,label=\"validation set\")\n",
    "plt.scatter(x_train, y_train, s=400,marker=r'$\\times$', label=\"training set\")\n",
    "plt.plot(x_grid, poly_reg.predict(X_poly), color = 'red', label=\"$r=4$\")\n",
    "plt.plot(x_grid_2, lin_reg.predict(x_grid.reshape(-1,1)), color = 'green', label=\"$r=1$\")\n",
    "plt.ylabel('label ' + r'$y$')\n",
    "plt.xlabel('feature ' + r'$x$')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5608ed9b712d8e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The above figure shows that the 4th degree polynomial (the red curve) fits the training data (orange crosses) almost perfectly. The average means-squared error incurred on the four training data points is essentially zero. However, it is quite clear that the polynomial fits the data outside of the training set very poorly.\n",
    "\n",
    "In contrast, the linear predictor provides a reasonable linear trend for the entire dataset, even though the fit on the training set is worse than that of the 4th degree polynomial.\n",
    "\n",
    "The phenomenon where a predictor has a very low error on the training set but generalizes poorly to other data from the same distribution is called **overfitting**. Thus, we can conclude that the 4th degree polynomial predictor **overfits** the training data. Due to the possibility of overfitting, we cannot be confident in that a predictor that fits the training data well is able to accurately predict the labels of new data points.\n",
    "\n",
    "The key idea of **validation** is to estimate the error of a predictor on data points that were not used for training the model. The validation error exposes models that overfit the training data, and thus it gives a more realistic estimate of the predictive capability of a model on new data points, in comparison to the training error. In the above figure, we could use the prediction error incurred for the data points marked by blue dots to validate the predictor functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49483a7aad5aa158",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "\n",
    "In this round you will learn a simple but powerful approach for choosing a \"good\" hypothesis space out of a set of alternatives. In particular, you will \n",
    "\n",
    "* learn that the training error is a poor quality measure for a hypothesis space \n",
    "* learn that the validation error is a more useful quality measure for a hypothesis space \n",
    "* learn how to choose between different hypothesis spaces (models) using the validation error\n",
    "* learn about regularization as a soft variant of model selection. \n",
    "\n",
    "## Background Material \n",
    "\n",
    "* [Video lecture](https://www.youtube.com/watch?v=MyBSkmUeIEs) of Prof. Andrew Ng on model validation and selection\n",
    "* [Short video](https://www.youtube.com/watch?v=TIgfjmp-4BA) on K-Fold Cross validation from Udacity\n",
    "* [Video lecture](https://www.youtube.com/watch?v=KvtGD37Rm5I) of Prof. Andrew Ng on regularization\n",
    "* Chapter 2; Chapter 6; Chapter 7 of this [tutorial](https://arxiv.org/abs/1805.05052)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90c66bf37e8ad022",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "## What is model validation?\n",
    "\n",
    "Suppose that we want to predict a numeric label (quantity of interest) $y \\in \\mathbb{R}$ based on some features $\\mathbf{x}=(x_{1},\\ldots,x_{n}) \\in \\mathbb{R}^{n}$ of a data point. In order to learn a good predictor $h(\\mathbf{x})$, we can use some data points $\\mathbb{X} = \\{ \\big( \\mathbf{x}^{(i)},y^{(i)}\\big)\\}$ for which we have determined the true label value $y^{(i)}$. Each data point in the training data $\\mathbb{X}$ is characterized by features $\\mathbf{x}^{(i)}$ and a label (quantity of interest) $y^{(i)}$. \n",
    "\n",
    "Consider a predictor $h(\\mathbf{x})$ which works extremely well on the dataset $\\mathbb{X}$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}}\\big(y^{(i)} - \\underbrace{h(\\mathbf{x}^{(i)})}_{= \\hat{y}^{(i)}}\\big)^{2}\\approx 0.\n",
    "\\end{equation}\n",
    "\n",
    "Even if the predictor $h(\\mathbf{x})$ does exceptionally well on the data set $\\mathbb{X}$, we can not be sure that the method will work well on new data points (different from the data points in $\\mathbb{X}$). \n",
    "This is particularly true for ML models that allow for complex predictor functions $h(\\mathbf{x})$. Examples of complex  predictor functions are linear functions $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} = \\sum_{r=1}^{n} x_{r} w_{r}$ using a large number of features $x_{1},\\ldots,x_{n}$ (the number $n$ of features is a measure of the complexity of the space of linear functions).\n",
    "\n",
    "Another example for a vast hypothesis space is given by the set of all predictor functions that can be represented by a given deep neural network structure with billions of adjustable weights (each edge has one weight $w$ that can be tuned). When using an extremely large hypothesis space $\\mathcal{H}$, it is very likely that just by chance one finds a predictor function $h(\\cdot) \\in \\mathcal{H}$ that perfectly fits (reproduces) a given set of labeled data points (unless this dataset is VERY large). \n",
    "\n",
    "It is worth emphasizing that the optimal complexity of the predictor function is typically dependent on the size of the dataset. A deep neural network might generalize well when trained on a huge dataset, whereas even a linear model with many features might be prone to severe overfitting on a small dataset. In particular, it can be shown that if the number of features is equal to the number of datapoints, a linear model can fit the data perfectly.\n",
    "\n",
    "ML methods that perform well on training data due to memorization of the training data do not pick up any intrinsic relation between features $\\mathbf{x}$ and label $y$. Such an ML method merely overfits the training data and will not be able to **generalize well** to new data. \n",
    "\n",
    "In order to detect overfitting we need to implement some form of **validation**. The idea behind validation is quite simple: \n",
    "\n",
    "**Split the available labeled data points $\\mathbb{X}$ into two different subsets, a training set $\\mathbb{X}^{(t)}$ of size $m_{t}$ and a validation set $\\mathbb{X}^{(v)}$ of size $m_{v}$.** \n",
    "\n",
    "<img src=\"../../data/R4_ModelValSel/SplitValTrain.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-714536d18e8f3dff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='splitTestandValidationfunction'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<b>Demo.</b> Split Data into Training and Validation Set.\n",
    "\n",
    "The code snippet below creates a synthetic dataset of $m$ datapoints $(\\mathbf{x}^{(i)},y^{(i)})$. Each data point is characterized by the feature vector $\\mathbf{x}^{(i)}=\\big(x^{(i)}_{1},\\ldots,x_{n}^{(i)}\\big)^{T} \\in \\mathbb{R}^{n}$ and a numeric label $y^{(i)} \\in \\mathbb{R}$. The feature vectors are stored in the rows of the matrix $\\mathbf{X}\\in \\mathbb{R}^{m \\times n}$. The labels are collected into the vector $\\mathbf{y}=\\big(y^{(1)},\\ldots,y^{(m)}\\big)^{T} \\in \\mathbb{R}^{m}$. \n",
    "\n",
    "The Python library `scikit-learn` provides the function \n",
    "\n",
    "`X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=2)` \n",
    "\n",
    "which can be used to split a dataset into training and validation set. The function reads in the feature vectors in the numpy array `X` of shape ($m,n$) and the labels in the numpy array `y` of shape ($m,$). \n",
    "\n",
    "The function returns numpy arrays `X_train` of shape ($m_{t},n$), `X_val`of shape ($m_{v},n$), `y_train` of shape ($m_{t},$) and `y_val` of shape ($m_{v},$). The input parameter `test_size` specifies the relative size $m_{v}/m$ of the validation set. When using `test_size=0.2`, $20 \\%$ of the original data points are used for the validation set and the remaining $80 \\%$ in the training set.\n",
    "\n",
    "[Scikit-learn documentation of train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b8b4f6fb62eef46b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split    # Import train_test_split function\n",
    "from sklearn import metrics\n",
    "\n",
    "m = 20    # Number of data points\n",
    "n = 10    # Number of features\n",
    "\n",
    "np.random.seed(4)    # Set random seed for reproduceability\n",
    "\n",
    "X = np.random.randn(m,n)    # create feature vectors using random numbers\n",
    "y = np.random.randn(m)    # create labels using random numbers \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)  # Split dataset with 80% training and 20% test\n",
    "\n",
    "plt.rc('legend', fontsize=14)    #  Set font size for legends\n",
    "plt.rc('axes', labelsize=14)    #  Set font size for axis labels\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10,10))    # Create figure with two subplots\n",
    "axes[0].set_title('Scatterplot of the entire dataset', fontsize=16)\n",
    "\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c='g',marker ='x', s=80, label='original dataset')  # Scatter plot of the original dataset\n",
    "axes[0].legend(loc='best')    # Set legend and set it in the best (automatically determined) position\n",
    "axes[0].set_xlabel(r'feature $x_1$')    # Set the label of the x-axis\n",
    "axes[0].set_ylabel(r'feature $x_2$')    # Set the label of the y-axis\n",
    "\n",
    "axes[1].scatter(X_train[:, 0], X_train[:, 1], c='g', marker ='o', s=80, label='training set')  # Scatter plot of the training set\n",
    "axes[1].scatter(X_val[:, 0], X_val[:, 1], c='brown', marker ='s', s=80, label='validation set')  # Scatter plot of the validation set\n",
    "axes[1].set_title('Training and validation sets', fontsize=16)\n",
    "axes[1].legend(loc='best')    # Set legend and set it in the best (automatically determined) position\n",
    "axes[1].set_xlabel(r'feature $x_1$')    # Set the label of the x-axis\n",
    "axes[1].set_ylabel(r'feature $x_2$')    # Set the label of the y-axis\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d82c01565964839f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The training set $\\mathbb{X}^{(t)}$ is used to learn the optimal predictor $h_{\\rm opt} \\in \\mathcal{H}$ out of the hypothesis space: \n",
    "\n",
    "\\begin{equation} \n",
    "h_{\\rm opt}  = {\\rm argmin}_{h \\in \\mathcal{H}} \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - \\underbrace{h(\\mathbf{x}^{(i)})}_{= \\hat{y}^{(i)}}\\big)^{2}. \n",
    "\\end{equation} \n",
    "\n",
    "The minimum objective value of this optimization problem is the **training error** \n",
    "\n",
    "\\begin{equation}\n",
    "E_{\\rm train} = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}.\n",
    "\\end{equation} \n",
    "\n",
    "Note that the training error $E_{\\rm train}$ measures the performance of the predictor $h_{\\rm opt}$ on the same data points $\\mathbb{X}^{(t)}$ which have been used to tune (learn) $h_{\\rm opt}$. Therefore, the training error $E_{\\rm train}$ is too **optimistic** as an estimate for the average error (or loss) of $h_{\\rm opt}$ on new data points which are different from $\\mathbb{X}^{(t)}$. \n",
    "\n",
    "To estimate the error incurred by $h_{\\rm opt}$ on new data points, we calculate the average loss incurred by $h_{\\rm opt}$ on the validation set $\\mathbb{X}^{(v)}$. This yields the **validation error**\n",
    "\n",
    "\\begin{equation}\n",
    "E_{\\rm val} = (1/m_{v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}. \n",
    "\\end{equation}\n",
    "\n",
    "The validation error $E_{\\rm val}$ is a much better estimate for the average error (or loss) of the predictor $h_{\\rm opt}$. \n",
    "\n",
    "The training error $E_{\\rm train}$ provides a quality measure for the particular predictor $h_{\\rm opt}$. In contrast, the validation error $E_{\\rm val}$ provides a quality measure for the entire hypothesis space $\\mathcal{H}$. Therefore, we can use the validation error for **model selection**. In model selection, we choose the best hypothesis space $\\mathcal{H}$ out of a set of alternative hypothesis spaces $\\mathcal{H}^{(1)},\\mathcal{H}^{(2)},\\ldots$ by selecting the one that achieves the lowest validation error $E_{\\rm val}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de358afd2c4fac99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Problem \n",
    "\n",
    "Model validation and selection is best understood by working through a particular example. To this end, we revisit the problem of predicting the grayscale value $y$ of a pixel in an aerial photograph. In **Round 2 - Regression**, we have formalized the grayscale value prediction as an ML problem with\n",
    "\n",
    "1. **data points** which represent pixels in the photograph. Each data point is characterized by features $\\mathbf{x} = (x_{1},\\ldots,x_{n}) \\in \\mathbb{R}^{n}$. Moreover, we define the grayscale value of the pixel as the label $y$ of the data point. \n",
    "\n",
    "2. a **hypothesis space** $\\mathcal{H}$ consisting of predictor functions $h: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ from features $\\mathbf{x} \\in \\mathbb{R}^{n}$ to a predicted grayscale value $\\hat{y}=h(\\mathbf{x})\\in \\mathbb{R}$ and \n",
    "\n",
    "3. a **loss function**, such as squared error loss, which measures the quality of a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d2949507bb9fa14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='handsondata'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<p><b>Demo.</b> Loading the Data.</p>\n",
    "    \n",
    "The following code snippet defines a function `X,y = GetFeaturesLabels(m,n)` which reads in the features and labels of pixels which are not corrupted (not fully black). The input parameters are the number `m` of data points and the number `n` of features to be used for each data point. The function returns a matrix $\\mathbf{X}$ and vector $\\mathbf{y}$. \n",
    "\n",
    "The features $\\mathbf{x}^{(i)}$ of data points are stored in the rows of the numpy array `X` (of shape (m,n)) and the corresponding grayscale values $y^{(i)}$ in the numpy array `y` (of shape (m,1)). The two arrays represent the feature matrix $\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}^{(1)} & \\ldots & \\mathbf{x}^{(m)} \\end{pmatrix}^{T}$ and the label vector $\\mathbf{y} = \\big( y^{(1)}, \\ldots, y^{(m)} \\big)^{T}$. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-711d85b7cf810763",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fbcf41691944>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# the library \"cv2\" provides powerful methods for image processing and computer vision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# import functions for displaying and plotting data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Pandas provides functions for loading (storing) data from (to) files\n",
    "import pandas as pd  \n",
    "# the library \"cv2\" provides powerful methods for image processing and computer vision\n",
    "import cv2 \n",
    "# import functions for displaying and plotting data \n",
    "from matplotlib import pyplot as plt \n",
    "from IPython.display import display, HTML\n",
    "# library \"numpy\" provides matrix (represented by numpy arrays) operations \n",
    "import numpy as np   \n",
    "# library \"random\" provides functions for generating random numbers\n",
    "import random\n",
    "\n",
    "def GetFeaturesLabels(m, n):\n",
    "    \n",
    "    # m - number of data points (pixels)\n",
    "    # n - number of features\n",
    "    \n",
    "    # filename of image file containing corrupted pixels\n",
    "    corrupted = '../../data/R2_Regression/SomePhotoCorrupted.bmp'\n",
    "    \n",
    "    # read corrupted image as numpy array\n",
    "    Photo = cv2.imread(corrupted, 0)\n",
    "    # set image size (100 by 100 pixels)\n",
    "    Photo = cv2.resize(Photo, (100, 100))\n",
    "    \n",
    "    # get image height and width \n",
    "    imgheight = Photo.shape[0]\n",
    "    imgwidth = Photo.shape[1]\n",
    "\n",
    "    # determine \"uncorroputed pixels\" by finding indices of those pixels with grayscale value larger than 0\n",
    "    good_idx = np.where(Photo > 0)\n",
    "\n",
    "    # store the vertical coordinate (row index) of uncorroputed pixels in numpy array `rows`\n",
    "    rows = good_idx[0] \n",
    "    # store the horizontal coordinate (column index) of uncorroputed pixels in numpy array `cols` \n",
    "    cols = good_idx[1]\n",
    "    \n",
    "    # set pads for defining pixel neighborhood and augmenting the image\n",
    "    wp = 1\n",
    "    hp = 1\n",
    "\n",
    "    # augment image with stripes such that we can also define neighborhoods of border pixels \n",
    "    # the values of these pixels are zero\n",
    "    tmp = np.vstack((np.zeros((wp, imgwidth)), Photo, np.zeros((wp, imgwidth))))\n",
    "    augmented = np.hstack((np.zeros((2*wp + imgheight, hp)), tmp, np.zeros((2*wp + imgheight, hp))))\n",
    "\n",
    "    # initialize feature vectors `x1`, `x2`and label vector `y` as numpy arrays \n",
    "    x1 = np.zeros((m,1))\n",
    "    #x2 = np.zeros((m,1))\n",
    "    y = np.zeros((m,1))\n",
    "    \n",
    "    # calculate the mean and median gray scale value of a pixel neighborhood \n",
    "    # here we define 3x3 pixel matrix surrounding a pixel as its neighborhood \n",
    "    for iter_datapoint in range(m):\n",
    "        row = rows[iter_datapoint] + wp # add wp to get the index of same data point in augmented Photo\n",
    "        col = cols[iter_datapoint] + hp # add hp to get the index of same data point in augmented Photo\n",
    "\n",
    "        # get the true label (gray scale value) of a datapoint (pixel)\n",
    "        y[iter_datapoint] = augmented[row, col]\n",
    "\n",
    "        # get values of pixel with its neighborhood (3x3 matrix) from image\n",
    "        neighbors = np.copy(augmented[(row-wp):(row+wp+1), (col-hp):(col+hp+1)])\n",
    "        # set value of a data point to 0 in order to exlude this value from calculation\n",
    "        # for the 3x3 array the indices for this data point(center of the neighborhood) is [1,1]\n",
    "        neighbors[1,1] = 0\n",
    "        \n",
    "        # calculate the feature of a data point (pixel) - the mean and median gray level of the neighborhoud\n",
    "        # zero values are exluded from calculation\n",
    "        x1[iter_datapoint] = np.mean(neighbors[neighbors != 0])   \n",
    "        #x2[iter_datapoint] = np.median(neighbors[neighbors != 0]) \n",
    "        \n",
    "    np.random.seed(30) # this is done so that every time that below np.random.randn is called, it produces the same output. \n",
    "                       # this is needed for testing purposes\n",
    "    # lets add some \"extra features\" here \n",
    "    X = np.hstack((x1, np.random.randn(n,m).T)) \n",
    "    \n",
    "    X = X[:,:n]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ddfcf8b06137c570",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear Predictors \n",
    "\n",
    "To predict the grayscale value $y$ of a pixel based on the first $r$ features (or characteristics) $\\mathbf{x}=(x_{1},\\ldots,x_{r})^{T} \\in \\mathbb{R}^{r}$, we try to find (or learn) a predictor function $h(\\mathbf{x})$ such that $y \\approx h(\\mathbf{x})$. We restrict ourselves to linear predictor functions without an intercept term. Thus, we use the hypothesis space \n",
    "\n",
    "$$ \\mathcal{H}^{(r)} = \\{ h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} \\mbox{ with some weight } \\mathbf{w}\\in \\mathbb{R}^{r} \\}.$$ \n",
    "\n",
    "Carefully note that for each value $r\\in \\{1,\\ldots,n\\}$, we obtain a different hypothesis space $\\mathcal{H}^{(r)}$ (or \"model\"). These hypothesis spaces are nested such that\n",
    "\n",
    "$$\\mathcal{H}^{(1)} \\subseteq \\mathcal{H}^{(2)} \\subseteq \\mathcal{H}^{(3)} \\ldots .$$\n",
    "\n",
    "This means that the hypothesis space $\\mathcal{H}^{(i)}$ contains all functions in the hypothesis spaces $\\mathcal{H}^{(j)}$, for $j=1,\\ldots,i-1$.\n",
    "\n",
    "For a fixed model parameter $r$, the weight vector $\\mathbf{w} \\in \\mathbb{R}^{r}$ is tuned by minimizing the average squared error loss incurred on the labeled data points in the training set $\\mathbb{X}^{(t)}$: \n",
    "\n",
    "\\begin{align}\\min_{h \\in \\mathcal{H}^{(r)}}  & \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}}  (y^{(i)} - h(\\mathbf{x}^{(i)}) )^{2} \\nonumber \\\\ \n",
    "= \\min_{\\mathbf{w} \\in \\mathbb{R}^{r}} & \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}}  \\big(y^{(i)} -  \\mathbf{w}^{T}\\mathbf{x}^{(i)}  \\big)^{2}.\n",
    "\\end{align}\n",
    "\n",
    "Solving this training problem provides us with optimal choices for weight vector $\\mathbf{w}$. \n",
    "However, we have another design parameter at our disposal: the number $r$ of features! While each pixel is characterized by $n$ features in our dataset, we are free to use fewer e.g. only the first $r \\leq n$ of these features. \n",
    "\n",
    "What is the best choice for $r$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0399c961c185f2f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Wrong Way \n",
    "\n",
    "Let us try out each hypothesis space $\\mathcal{H}^{(r)}$ on the training data $\\mathbb{X}^{(t)}$. For each $r=1,\\ldots,h,$ we find the optimal predictor $h_{\\rm opt}^{(r)} \\in \\mathcal{H}^{(r)}$ by minimizing the average loss\n",
    "\n",
    "\\begin{align} \n",
    "\\mathcal{E}(r) & = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big( y^{(i)}- h^{(r)}(\\mathbf{x}^{(i)}) \\big)^{2}. \n",
    "\\end{align} \n",
    "\n",
    "The training error for the hypothesis space $\\mathcal{H}^{(r)}$ is then calculated as the mean-squared error incurred by the optimal predictor $h_{\\rm opt}^{(r)}$:\n",
    "\n",
    "\\begin{align} \n",
    "E_{\\rm train}(r) & = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big( y^{(i)}- h_{\\rm opt}^{(r)}(\\mathbf{x}^{(i)}) \\big)^{2} \\nonumber \\\\ \n",
    "& = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big( y^{(i)}- \\mathbf{w}_{\\rm opt}^T \\mathbf{x}^{(i)}) \\big)^{2}. \\nonumber \n",
    "\\end{align} \n",
    "\n",
    "We can see that when the loss function is the mean-squared error - such as in ordinary least squares regression - the loss of the optimal predictor is equivalent to the training error. However, this is not always the case as we will see later in this notebook when considering regularization. \n",
    "\n",
    "It is tempting to choose the number $r$ of features according to the smallest training error $E_{\\rm train}(r)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0aaf4cb2c4109eb2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='trainModel'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<p><b>Demo.</b> Varying Number of Features </p>\n",
    "    \n",
    "The following code snippet computes the training error $E_{\\rm train}(r)$ for each choice of $r$. For each particular value $r=1,\\ldots,n$, the best linear predictor $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$ is found using the  function `.fit()` of the `LinearRegression` class in scikit-learn.\n",
    "\n",
    "[Documentation of the LinearRegression class in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) \n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-acd5c9243afcd36f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "m = 20                        # we use the first m=20 data points (pixels) from the aerial photo \n",
    "n = 10                        # maximum number of features used \n",
    "\n",
    "X,y = GetFeaturesLabels(m,n)  # read in m data points using n features \n",
    "linreg_error = np.zeros(n)    # vector for storing the training error of LinearRegresion.fit() for each r\n",
    "\n",
    "for r_minus_1 in range(n):    # loop r times\n",
    "    reg = LinearRegression(fit_intercept=False)    # create an object for linear predictors\n",
    "    reg = reg.fit(X[:,:(r_minus_1 + 1)], y)    # find best linear predictor (minimize training error)\n",
    "    pred = reg.predict(X[:,:(r_minus_1 + 1)])    # compute predictions of best predictors \n",
    "    linreg_error[r_minus_1] = mean_squared_error(y, pred)    # compute training error \n",
    "\n",
    "plot_x = np.linspace(1, n, n, endpoint=True)    # plot_x contains grid points for x-axis (1,...,n)\n",
    "\n",
    "# Plot training error E(r) as a function of feature number r\n",
    "plt.rc('legend', fontsize=14)    #  Set font size for legends\n",
    "plt.rc('axes', labelsize=14)    #  Set font size for axis labels\n",
    "plt.figure(figsize=(10,6))    # Set figure size\n",
    "plt.plot(plot_x, linreg_error, label='$E(r)$', color='red')\n",
    "plt.xlabel('# of features $r$')\n",
    "plt.ylabel('training error $E(r)$')\n",
    "plt.title('training error vs number of features', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fba2c040fb160f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Let's Interpret the Results!\n",
    "\n",
    "Based on the above plot, we could argue that we should choose the linear model with $r=10$ features since this yields the lowest training error $E(r)$. **This reasoning is incorrect** since our ultimate goal is to find a predictor for new pixels for which we do not know the grayscale values (e.g. corrupted pixels). Our goal is not to accurately reproduce the grayscale values of pixels for which we already know these values! \n",
    "\n",
    "Using the training error $E_{\\rm train}(r)$ to assess the quality of the predictor $h_{\\rm opt}^{(r)}$ is misleading since $h_{\\rm opt}^{(r)}$ is based on the weight vector $\\mathbf{w}$ that is perfectly tuned to the training data $\\mathbb{X}^{(t)}$. Also, the more features (larger $r$) we use, the better we will be able to fit the training data $\\mathbb{X}^{(t)}$ (obtain smaller training error). However, this does not necessarily lead to better performance on new data. A complex model with too many features (large $r$) might only fit the training data very well, and generalize poorly to new data.\n",
    "\n",
    "Consider the case of $r=m_{\\rm train}$, i.e., the number of features is the same as the number of labeled data points in the training set. Under very mild conditions it can be shown that in this case there always exists a linear predictor $h(\\mathbf{x})=\\mathbf{w}^{T} \\mathbf{x}$ such that $y^{(i)} = h(\\mathbf{x}^{(i)})$, i.e., the training error is exactly zero (see Chapter 7.1 of the coursebook)! \n",
    "A better way to evaluate the quality of a predictor is presented next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e508ee65304e99f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  The Right Way\n",
    "\n",
    "The training error $E_{\\rm train}(r)$ is a bad measure for the quality of a hypothesis space $\\mathcal{H}^{(r)}$ since it will always favor larger spaces (larger number $r$ of features). A more useful measure for the quality of a hypothesis space $\\mathcal{H}^{(r)}$ is the validation error \n",
    "\n",
    "\\begin{equation}\n",
    "E_{\\rm val}(r) = (1/m_{v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h^{(r)}_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}, \n",
    "\\end{equation} \n",
    "\n",
    "where the predictor $h_{\\rm opt}^{(r)}(\\mathbf{x}) = \\mathbf{w}_{\\rm opt}^T\\mathbf{x}$ is obtained by minimizing the training error $E_{\\rm train}(r)$ with respect to $\\mathbf{w}$. \n",
    "\n",
    "Since a lower validation error corresponds to better predictions, the best hypothesis space for our problem is the one with the lowest validation error. Consequently, we should choose the model with the lowest validation error when performing model selection in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a7b319a106cbd05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='splitTestandValidationfunction'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Generate Training and Validation Set.\n",
    "   \n",
    "Use the `scikit-learn` library function `train_test_split()` to split the data points obtained from the function `GetFeaturesLabels` into a training and validation set. The function should be used with the choice `random_state=2` and `test_size=0.2`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cc67382efb4ce19",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    # Import train_test_split function\n",
    "\n",
    "m = 20    # we use the first m=20 data points (pixels) from the aerial photo\n",
    "n = 10    # maximum number of features used \n",
    "\n",
    "X, y = GetFeaturesLabels(m,n)    # read in m data points using n features \n",
    "\n",
    "### STUDENT TASK ###\n",
    "# Compute the training and validation sets\n",
    "# X_train, X_val, y_train, y_val = ...\n",
    "### BEGIN SOLUTION\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)  # 80% training and 20% test\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2fbabdcefb77f271",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check dimensions of train and validation vectors\n",
    "assert len(X_train) == 16, \"The 'x_train' vector has the wrong length\"\n",
    "assert len(y_train) == 16, \"The 'y_train' vector has the wrong length\"\n",
    "assert len(X_val) == 4,   \"The 'x_val' vector has the wrong length\"\n",
    "assert len(y_val) == 4, \"The 'y_val' vector has the wrong length\"\n",
    "print('Sanity checks passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-428dadef568d5aef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='trainValErrorsfunction'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Compute Training and Validation Error. \n",
    "\n",
    "**(1)** Complete the function `get_train_val_errors(X_train, X_val, y_train, y_val, n_features)` that returns the training error and validation error for each choice of $r=1,\\ldots,n$. Please use `fit_intercept=False`. The training errors should be stored in a numpy array `err_train` of shape (n,1) and the validation errors should be stored in the numpy array `err_val` of shape (n,1). The first entries of `err_train` and `err_val` should be $E_{\\rm train}(1)$ and $E_{\\rm val}(1)$. \n",
    "\n",
    "**(2)** Complete the function `get_best_model(err_val)`, that takes as input the validation errors `err_val` for each number of features $r=1,\\ldots,n$ and returns the optimum number $\\hat{r}$ of features (such that the validation error is smallest). \n",
    "\n",
    "Hint: you can determine the index of the smallest entry in a numpy array using `np.argmin()` ([see documentation](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.argmin.html)).\n",
    "\n",
    "**IMPORTANT!**: Remember that indexing for numpy arrays starts with index 0. However, we start with model size $r=1$. Thus, you need to add 1 to the index that you get by using `np.argmin()`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7791084dee96529b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_errors(X_train, X_val, y_train, y_val, n_features):  \n",
    "    err_train = np.zeros((n,1))    # Array for storing training errors\n",
    "    err_val = np.zeros((n,1))    # Array for storing validation errors\n",
    "    \n",
    "    for r_minus_1 in range(n):    # Loop over the number of features r (minus one)\n",
    "        ### STUDENT TASK ###\n",
    "        ### BEGIN SOLUTION\n",
    "        lin_reg = LinearRegression(fit_intercept=False)\n",
    "        lin_reg = lin_reg.fit(X_train[:,:(r_minus_1+1)], y_train)\n",
    "        w_opt = lin_reg.coef_\n",
    "        y_pred_train = lin_reg.predict(X_train[:,:(r_minus_1+1)])\n",
    "        err_train[r_minus_1] = mean_squared_error(y_train, y_pred_train)\n",
    "        y_pred_val = lin_reg.predict(X_val[:,:(r_minus_1+1)])\n",
    "        err_val[r_minus_1] = mean_squared_error(y_val, y_pred_val)\n",
    "        ### END SOLUTION\n",
    "    return err_train, err_val\n",
    "\n",
    "def get_best_model(err_val):\n",
    "    # best_model = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    best_model = np.argmin(err_val) + 1\n",
    "    ### END SOLUTION\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c1430b345ba97bfc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate training and validation errors using ´get_train_val_errors´\n",
    "err_train, err_val = get_train_val_errors(X_train, X_val, y_train, y_val, n)\n",
    "\n",
    "# Perform some sanity checks on the results\n",
    "assert err_train.shape == (n,1), \"numpy array err_train has wrong shape\"\n",
    "assert err_val.shape == (n,1), \"numpy array err_val has wrong shape\"\n",
    "print('Sanity checks passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "def test_get_train_val_errors(X_train, X_val, y_train, y_val, n_features):  \n",
    "    err_train = np.zeros([n,1]) # Array for storing training errors\n",
    "    err_val = np.zeros([n,1]) # Array for storing validation errors\n",
    "    \n",
    "    for r_minus_1 in range(n):\n",
    "        lin_reg = LinearRegression(fit_intercept=False)\n",
    "        lin_reg = lin_reg.fit(X_train[:,:(r_minus_1+1)], y_train)\n",
    "        w_opt = lin_reg.coef_\n",
    "        y_pred_train = lin_reg.predict(X_train[:,:(r_minus_1+1)])\n",
    "        err_train[r_minus_1] = mean_squared_error(y_train, y_pred_train)\n",
    "        y_pred_val = lin_reg.predict(X_val[:,:(r_minus_1+1)])\n",
    "        err_val[r_minus_1] = mean_squared_error(y_val, y_pred_val)\n",
    "    return err_train, err_val\n",
    "\n",
    "t_err_train, t_err_val = test_get_train_val_errors(X_train, X_val, y_train, y_val, n)\n",
    "\n",
    "np.testing.assert_allclose(t_err_train.reshape(-1,1), err_train.reshape(-1,1), atol=1e-3, err_msg='get_train_val_errors is not correctly calculating the training errors')\n",
    "np.testing.assert_allclose(t_err_val.reshape(-1,1), err_val.reshape(-1,1), atol=1e-3, err_msg='get_train_val_errors is not correctly calculating the validation errors')\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b6e6c7e09a33407f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the best model using `get_best_model`\n",
    "best_model = get_best_model(err_val)\n",
    "\n",
    "# Print the best model\n",
    "print('The best model is obtained for r={}'.format(best_model))\n",
    "\n",
    "# Perform some sanity checks on the result\n",
    "assert best_model != None, \"Please choose a value between 1 and n \"\n",
    "assert best_model <= n, \"The values should be less than n\"\n",
    "assert best_model > 0, \"The values should be more than 0\"\n",
    "print('Sanity checks passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "def test_get_train_val_errors(X_train, X_val, y_train, y_val, n_features):  \n",
    "    err_train = np.zeros([n,1]) # Array for storing training errors\n",
    "    err_val = np.zeros([n,1]) # Array for storing validation errors\n",
    "    \n",
    "    for r_minus_1 in range(n):\n",
    "        lin_reg = LinearRegression(fit_intercept=True)\n",
    "        lin_reg = lin_reg.fit(X_train[:,:(r_minus_1+1)], y_train)\n",
    "        w_opt = lin_reg.coef_\n",
    "        y_pred_train = lin_reg.predict(X_train[:,:(r_minus_1+1)])\n",
    "        err_train[r_minus_1] = mean_squared_error(y_train, y_pred_train)\n",
    "        y_pred_val = lin_reg.predict(X_val[:,:(r_minus_1+1)])\n",
    "        err_val[r_minus_1] = mean_squared_error(y_val, y_pred_val)\n",
    "    return err_train, err_val\n",
    "\n",
    "t_err_train, t_err_val = test_get_train_val_errors(X_train, X_val, y_train, y_val, n)\n",
    "t_best_model = np.argmin(t_err_val) + 1\n",
    "\n",
    "assert best_model == t_best_model\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-802d3729eb036c9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we plot the training and validation errors from the previous task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64473b41dbc92dfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation errors for the different number of features r\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, n + 1), err_train, color='black', label=r'$E_{\\rm train}(r)$', marker='o')  # Plot training error\n",
    "plt.plot(range(1, n + 1), err_val, color='red', label=r'$E_{\\rm val}(r)$', marker='x')  # Plot validation error\n",
    "\n",
    "plt.title('Training and validation error for different number of features', fontsize=16)    # Set title\n",
    "plt.ylabel('Empirical error')    # Set label for y-axis\n",
    "plt.xlabel('r features')    # Set label for x-axis\n",
    "plt.xticks(range(1, n + 1))  # Set the tick labels on the x-axis to be 1,...,n\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61ecf20008a16c4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "From the figure, we see that the training error is monotonically decreasing with an increasing number $r$ of features used in the linear predictor $h(\\mathbf{x}) = w_{1}x_{1}+\\ldots+w_{r}x_{r}$. However, the validation error is first decreasing but then rapidly increases for larger values of $r$. It is clear that the training error can be very misleading as a measure for prediction error on new data for large values of $r$, which correspond to more complex models.\n",
    "\n",
    "Some questions arise when observing the above figure. How come the validation error is lower than the training error when $r < 5$, and why does the validation error drop for $r=10$ features? \n",
    "\n",
    "The reason for these apparent anomalies turns out to be pure randomness. Using only a single split of the data into training and validation set bears the risk of being extremely \"unlucky\", in the sense that the single split might result in a highly non-typical validation set such that the validation error is not a reliable measure for the average error on new data. This problem is particularly prevalent when using small datasets, such as in this exercise ($m=20$).\n",
    "\n",
    "In our case, the validation set happens to fit the optimal predictors for $r < 5$ too well by chance. As a result, the resulting validation errors are unrealistically low and do not give a reliable estimate of the error of the model on new data. Equally well, we could have obtained a validation set that gives excessively high validation errors and therefore pessimistic estimates of the generalization capabilities of the model. \n",
    "\n",
    "Next we will consider $K$-fold cross-validation, which is a straightforward extension to the \"single-split approach\" that increases the robustness of the validation error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a417d9495eb3f40e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## K-fold Cross-Validation\n",
    "\n",
    "In general, there is no unique optimal way of splitting a data set into a training and validation set. The precise choice of how to divide data points into the training and validation set and also their relative size (80/20, 50/50 ...) has to be considered case-by-case for the application at hand. \n",
    "\n",
    "To get more guidance on how to split the data, one typically needs to have additional knowledge about the statistical properties of the data generating process. An accurate probabilistic model for the data points allows determining optimal split ratios between the training and validation set. That said, probabilistic (generative) models for the observed data points is beyond the scope of this course. \n",
    "\n",
    "$K$-fold cross-validation randomly splits the data into $K$ equal-sized subsets (\"folds\"). It then executes $K$ rounds, each round corresponding to one of the $K$ folds. In the $k$th round, the $k$th fold is used as the validation set and the remaining $K-1$ folds are used as the training set. The validation errors obtained during each fold are then averaged to obtain the final validation error. \n",
    "\n",
    "As an example, a diagram of  5-fold cross-validation is depicted below. For each round, the fold which is used as the validation set is indicated by \"test\". \n",
    "\n",
    "![Components](cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfold'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<p><b>Demo.</b> Splitting data into K-folds in sklearn.</p>\n",
    "    \n",
    "The code snippet below shows how to use a `KFold` object in scikit-learn to iterate through `K` train/validation splits of the dataset `X`.\n",
    "    \n",
    "On initialization the `KFold` object is given the number of data splits `K` as an argument to the parameter `n_splits`. The Python [generator function](https://docs.python.org/3.8/glossary.html#term-generator) `KFold.split(X)` can then be used to iterate through the pairs of training and validation indices. \n",
    "\n",
    "For an array `idx` of indices, the data points in X corresponding to these indices can be obtained by `X[idx,:]`. We can use this to obtain the training and validation sets given the indices of the datapoints in the respective sets.\n",
    "\n",
    "For more information, see the scikit-learn [documentation of KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KFold class from scikitlearn library\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K=5    # Specify the number of folds of split data into\n",
    "kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "# For all splits, print the validation and training indices\n",
    "iteration = 0\n",
    "for train_indices, test_indices in kf.split(X):\n",
    "    iteration += 1\n",
    "    X_train = X[train_indices,:]    # Get the training set    \n",
    "    X_val = X[test_indices,:]    # Get the validation set\n",
    "    print('Iteration {}:'.format(iteration))\n",
    "    print('Indices for validation set:', test_indices)\n",
    "    print('Indices for training set:', train_indices)\n",
    "    print('X_val shape: {}, X_train shape: {} \\n'.format(X_val.shape, X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13d40bbc019a9337",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='kfold'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student task.</b> 5-Fold Cross Validation.</p>\n",
    "     \n",
    "The purpose of the code snippet below is to compute the training and validation errors for each choice of $r=1,\\ldots,n$ using 5-fold cross-validation. Your task is to complete the part of the loop that performs 5-fold cross-validation using the `KFold` class in scikit-learn. For each $r$ you should\n",
    "    \n",
    "1. Iterate over the `K` pairs of train and test indices and for each pair, calculate the training and validation errors of a linear regression model (with `fit_intercept=False`) and store them in  `train_errors_per_cv_iteration` and `test_errors_per_cv_iteration` respectively.\n",
    "    \n",
    "    \n",
    "2. Calculate the average training- and validation errors and store these at index `r_minus_1` in the arrays `err_train` and `err_val` (both of shape $(n, )$) respectively.\n",
    "\n",
    "For more information, see the scikit-learn [documentation of KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "\n",
    "Afterwards, the training- and validation errors are plotted for comparison with the errors from the previous student task.\n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20    # we use the first m=20 data points (pixels) from the aerial photo \n",
    "n = 10\n",
    "\n",
    "X, y = GetFeaturesLabels(m,n)  # read in m data points with n features \n",
    "\n",
    "err_train = np.zeros(n)  # Array to store training errors\n",
    "err_val = np.zeros(n)  # Array to store validation errors\n",
    "\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "for r_minus_1 in range(n):\n",
    "    train_errors_per_cv_iteration = []  # List for storing the training errors for the splits\n",
    "    val_errors_per_cv_iteration = []  # List for storing the validation errors for the splits\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    ### BEGIN SOLUTION\n",
    "    for train_indices, test_indices in kf.split(X):\n",
    "        reg = LinearRegression(fit_intercept=False)    # Create new linear regression model\n",
    "        reg = reg.fit(X[train_indices,:(r_minus_1+1)], y[train_indices])    # Fit the model on the current training set\n",
    "        y_pred_train = reg.predict(X[train_indices,:(r_minus_1+1)])    # Calculate the predicted labels of the current training set\n",
    "        train_errors_per_cv_iteration.append(mean_squared_error(y[train_indices], y_pred_train))    # Add the training error to list of errors\n",
    "        y_pred_val = reg.predict(X[test_indices,:(r_minus_1+1)])    # Calculate the predicted labels of the current test set\n",
    "        val_errors_per_cv_iteration.append(mean_squared_error(y[test_indices], y_pred_val))    # Add the test error to list of errors\n",
    "\n",
    "    err_train[r_minus_1] = np.mean(train_errors_per_cv_iteration)    # compute the mean of round-wise training errors\n",
    "    err_val[r_minus_1] = np.mean(val_errors_per_cv_iteration)    # compute the mean of round-wise validation errors\n",
    "    ### END SOLUTION\n",
    "    \n",
    "print('Training errors for each K:')\n",
    "print(err_train, '\\n')\n",
    "print('Validation error for each K:')\n",
    "print(err_val, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sanity checks on the outputs\n",
    "assert err_train.shape == (n,), \"err_train is of the wrong shape!\"\n",
    "assert err_val.shape == (n,), \"err_val is of the wrong shape!\"\n",
    "assert err_val[0] < err_val[1], \"The second element of err_val should be larger than the first element!\"\n",
    "\n",
    "print(\"Sanity checks passed!\")\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "t_err_train = np.zeros(n)  # Array to store training errors\n",
    "t_err_val = np.zeros(n)  # Array to store validation errors\n",
    "\n",
    "K = 5\n",
    "t_kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "for r in range(n):\n",
    "    t_train_errors_per_cv_iteration = []  # List for storing the training errors for the splits\n",
    "    t_val_errors_per_cv_iteration = []  # List for storing the validation errors for the splits\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    ### BEGIN SOLUTION\n",
    "    for train_indices, test_indices in t_kf.split(X):\n",
    "        t_reg = LinearRegression(fit_intercept=False)    # Create new linear regression model\n",
    "        t_reg = t_reg.fit(X[train_indices,:(r+1)], y[train_indices])    # Fit the model on the current training set\n",
    "        t_y_pred_train = t_reg.predict(X[train_indices,:(r+1)])    # Calculate the predicted labels of the current training set\n",
    "        t_train_errors_per_cv_iteration.append(mean_squared_error(y[train_indices], y_pred_train))    # Add the training error to list of errors\n",
    "        t_y_pred_val = t_reg.predict(X[test_indices,:(r+1)])    # Calculate the predicted labels of the current test set\n",
    "        t_val_errors_per_cv_iteration.append(mean_squared_error(y[test_indices], y_pred_val))    # Add the test error to list of errors\n",
    "\n",
    "    t_err_train[r] = np.mean(train_errors_per_cv_iteration)    # compute the mean of round-wise training errors\n",
    "    t_err_val[r] = np.mean(val_errors_per_cv_iteration)    # compute the mean of round-wise validation errors\n",
    "    ### END SOLUTION\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation errors for the different number of features r\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, n+1), err_train, color='black', label=r'$E_{\\rm train}(r)$', marker='o')  # Plot training error\n",
    "plt.plot(range(1, n+1), err_val, color='red', label=r'$E_{\\rm val}(r)$', marker='x')  # Plot validation error\n",
    "\n",
    "plt.title('5-fold training and validation errors for different number of features', fontsize=16)    # Set title\n",
    "plt.ylabel('Empirical error')    # Set label for y-axis\n",
    "plt.xlabel('r features')    # Set label for x-axis\n",
    "plt.xticks(range(1, n + 1))  # Set the tick labels on the x-axis to be 1,...,n\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12f1478400587745",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If we compare the figure above to the one in the previous student task, we can see that the validation error obtained by 5-fold cross-validation seems to provide a more realistic estimate of the performance of the model on new data. The validation error is now consistently larger than the training error, and the rising trend of the validation error seems slightly smoother.\n",
    "\n",
    "In practice, it is almost always preferable to use K-fold cross-validation instead of a single validation split for model validation and selection due to the increased robustness of the validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a451b1c380ed82da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Regularization\n",
    "\n",
    "Consider a ML method based on a large hypothesis space such as linear predictors using many features or polynomials with a large degree. Large hypothesis spaces typically contain complex predictors that achieve very low training errors by overfitting the data. Thus, if we search for the optimal predictor in this hypothesis space (i.e train our model) by minimizing the training error, we will obtain a predictor that overfits the training data and  generalizes poorly to data that is not in the training set.\n",
    "\n",
    "One solution to prevent overfitting is to choose a model out of a selection of candidate models based on the validation error, like we did in the student task \"Compute Training and Validation Error\". By using the validation error as the selection criteria, we are able to select the model that performs best on new data. While this approach is useful, it can be very difficult to implement in settings where the number of feasible hypothesis spaces is very large.\n",
    "\n",
    "**Regularization** is a more sophisticated approach to prevent overfitting, and is based on the idea of **estimating the expected increase of the validation error (relative to the training error) incurred by more complex predictors**. When applying regularization, a large hypothesis space is used in conjunction with a loss function that penalizes the complexity of the predictor. If we recall that the optimal predictor is found by minimizing the loss function, it is clear that by choosing a loss function that penalizes complexity, the optimal predictor will be less complex. In practice, the penalization is done by adding a **regularization term** $\\mathcal{R}(h)$ to the training error: \n",
    "\n",
    "\\begin{equation}\n",
    "h^{(\\lambda)}_{\\rm opt}  = {\\rm argmin}_{h \\in \\mathcal{H}} \\underbrace{\\underbrace{(1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2}}_{\\mbox{ training error}} + \\underbrace{\\alpha \\mathcal{R}(h)}_{\\mbox{anticipated increase of error (loss) on new data}}}_{\\mbox{ estimate (approximation) of validation error }}.  \n",
    "\\end{equation}\n",
    "\n",
    "The regularization term $\\mathcal{R}(h)$ quantifies the anticipated increase in the validation error (compared to the training error) due to the \"complexity\" (e.g. the number of features used in a linear predictor) of a particular predictor. In a nutshell, the regularization term penalizes the use of more complex predictors and therefore favors \"simpler\" predictor functions. The precise meaning of \"complexity\" or \"simpler\" is determined by the (design) choice for the regularization term $\\mathcal{R}(h)$. \n",
    "\n",
    "Two widely used choices for measuring the complexity of linear predictors $h(\\mathbf{x}) = \\mathbf{w}^{T}\\mathbf{x}$ is the squared Euclidean norm $\\mathcal{R}(h) = \\|\\mathbf{w}\\|^{2}_{2}=\\sum_{r=1}^{n} w_{r}^{2}$ or the $\\ell_{1}$ norm $\\mathcal{R}(h) = \\|\\mathbf{w}\\|_{1}=\\sum_{r=1}^{n} |w_{r}|$. \n",
    "\n",
    "The regularization parameter $\\alpha$ **offers a trade-off between the prediction error (training error) incurred on the training data and the complexity of a predictor**. The larger we choose $\\alpha$, the more emphasis is put on obtaining \"simple\" predictor functions. Using very small values for $\\alpha$ prefers predictor functions which achieve a small training error (at the expense of being a more complicated function).\n",
    "\n",
    "In order to actually implement regularization, we need to \n",
    "- choose (define) the function $\\mathcal{R}(h)$ that quantifies some notion of complexity of a predictor function $h \\in \\mathcal{H}^{(n)}$. \n",
    "- choose the value of the regularization parameter $\\alpha$. \n",
    "\n",
    "A principled approach to these choices is to assume a probabilistic model for how the data points are generated. It is then possible to relate optimal choices for the function $\\mathcal{R}(h)$ and the value $\\alpha$ to the parameters of the probability distribution of the data points. However, probabilistic modelling in machine learning is beyond the scope of this course. \n",
    "\n",
    "Instead of probabilistic modelling, we can use again the concept of validation sets to find good choices for the regularization function and parameter. In particular, \n",
    "- we first specify a set of different choices for the function $\\mathcal{R}(h)$ and regularization parameter value $\\alpha$, \n",
    "- for each choice for $\\alpha$ and $\\mathcal{R}(h)$, we learn a predictor that minimizes the regularized training error \n",
    "\n",
    "\\begin{equation}\n",
    "h^{(\\alpha)}_{\\rm opt}  = {\\rm argmin}_{h \\in \\mathcal{H}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha \\mathcal{R}(h).    \n",
    "\\end{equation}\n",
    "- evaluate the resulting predictor $h^{(\\alpha)}_{\\rm opt}$ by computing the validation error\n",
    "\\begin{equation}\n",
    "E_{\\rm val}^{(\\alpha)} = (1/m_{\\rm v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h^{(\\alpha)}_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}.\n",
    "\\end{equation} \n",
    "\n",
    "We then use the regularization measure $\\mathcal{R}(h)$ and the value for $\\alpha$ with smallest validation error $E_{\\rm val}^{(\\alpha)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a697f48e6cf7e933",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='ridgeReg'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<p><b>Demo.</b> Ridge Regression. </p>\n",
    "\n",
    "Ridge regression learns a linear predictor functions $h^{(\\mathbf{w})}(\\mathbf{x}) =\\mathbf{w}^{T} \\mathbf{x}$ by minimizing the sum of training error and the scaled regularization term $\\mathcal{R}(h)=\\|\\mathbf{w}\\|_{2}^{2}$. A ridge regression model can be fitted to a dataset with scikit-learn by using the function `Ridge.fit()`. After fitting the model, the optimal weight vector $\\mathbf{w}_{\\rm opt}$ is stored in the attribute `Ridge.coef_` of the `Ridge` instance. \n",
    "\n",
    "[See documentation of Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef05953e6a07a985",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "m = 20\n",
    "n = 10\n",
    "X, y = GetFeaturesLabels(m, n)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)  # 80% training and 20% test\n",
    "\n",
    "alpha = 10    # Define value of the regularization parameter 'alpha'\n",
    "\n",
    "ridge = Ridge(alpha=alpha, fit_intercept=False)    # Create Ridge regression model\n",
    "ridge.fit(X_train, y_train)    # Fit the Ridge regression model on the training set\n",
    "y_pred = ridge.predict(X_train)    # Predict the labels of the training set\n",
    "w_opt = ridge.coef_    # Get the optimal weights (regression coefficients) of the fitted model\n",
    "err_train = mean_squared_error(y_pred, y_train)    # Calculate the training error\n",
    "\n",
    "# Print optimal weights and training error\n",
    "print('Optimal weights: \\n', w_opt)\n",
    "print('Training error: \\n', err_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Lasso** is another regularized regression method, which regularizes the training error of linear predictors $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$ with the complexity measure $\\mathcal{R}(h)= \\|\\mathbf{w}\\|_{1}$. In Lasso regression, it is customary to use a regularized loss function where the training error term is multiplied by $1/2$. That is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{E}(\\textbf{w}) = \\frac{1}{2m_t} \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha |\\textbf{w}|.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\alpha$ can be freely chosen, the multiplicative constant is of no practical importance and is only included in order to make analytical calculations more convenient. Still, the exact form of the loss function is important knowledge since this loss function is typically used in Lasso implementations, incuding the one in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b63b8bd46646688d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='lassoReg'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Lasso Regression.\n",
    "\n",
    "Complete the function `fit_lasso` that uses the Scikit-learn function `Lasso.fit()` to compute the optimal predictor for $\\alpha=$ `alpha_val`. When initializing Lasso, please use `fit_intercept=False`. This function is then used find the optimal Lasso predictor for $\\alpha = 10$.\n",
    "\n",
    "[Documentation for Lasso in Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6cd24ae92c6b545",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X,y = GetFeaturesLabels(m,n)    # read in m data points using n features \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)    # 80% training and 20% test\n",
    "\n",
    "def fit_lasso(X_train, y_train, alpha_val):\n",
    "    ### STUDENT TASK ###\n",
    "    # .\n",
    "    # .\n",
    "    # .\n",
    "    # w_opt = ...\n",
    "    # training_error = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    lasso = Lasso(alpha=alpha_val, fit_intercept=False)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    y_pred = lasso.predict(X_train)\n",
    "    w_opt = lasso.coef_\n",
    "    training_error = mean_squared_error(y_pred, y_train)\n",
    "    ### END SOLUTION\n",
    "    return w_opt, training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-44c7f2c09ba11e52",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set alpha value\n",
    "alpha_val = 10\n",
    "\n",
    "# Fit Lasso and calculate optimal weights and training error using the function 'fit_lasso'\n",
    "w_opt, training_error = fit_lasso(X_train, y_train, alpha_val)\n",
    "\n",
    "# Print optimal weights and the corresponding training error\n",
    "print('Optimal weights: \\n', w_opt)\n",
    "print('Training error: \\n', training_error)\n",
    "\n",
    "# Perform some sanity checks on the outputs\n",
    "from sklearn.linear_model import Lasso\n",
    "assert w_opt.reshape(-1,1).shape == (10,1), \"'w_opt' has wrong shape\"\n",
    "assert np.isscalar(training_error), \"'training_error' is not scalar\"\n",
    "assert training_error < 1000, \"'training_error' is too large\"\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "def t_fitLasso(x_train, y_train, lambd=0):\n",
    "    t_lasso = Lasso(alpha=lambd, fit_intercept=False)\n",
    "    t_lasso.fit(X_train, y_train)\n",
    "    y_pred = t_lasso.predict(X_train)\n",
    "    w_opt = t_lasso.coef_\n",
    "    training_error = mean_squared_error(y_pred, y_train)\n",
    "    return w_opt, training_error\n",
    "\n",
    "t_w_opt, t_training_error = t_fitLasso(X_train, y_train, lambd=10)\n",
    "\n",
    "np.testing.assert_allclose(t_w_opt.reshape(-1,1), w_opt.reshape(-1,1), rtol=1e-10, atol=0)\n",
    "np.testing.assert_almost_equal(t_training_error, training_error)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1355f27ba4fd6ca5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When using Lasso or ridge regression, we need to find a suitable value for the regularization parameter $\\alpha$. A simple but useful approach is **grid search**: We first specify a list of values to be used for the regularization parameter. For each value $\\alpha$, we determine a predictor $h^{(\\alpha)}$ by minimizing the regularized training error: \n",
    "\\begin{equation}\n",
    "h^{(\\alpha)}  = {\\rm argmin}_{h \\in \\mathcal{H}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha \\mathcal{R}(h).    \n",
    "\\end{equation}\n",
    "The resulting training error is \n",
    "\\begin{equation} \n",
    "E_{\\rm train}(\\alpha) = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h^{(\\alpha)}(\\mathbf{x}^{(i)}) \\big)^{2}. \n",
    "\\end{equation}\n",
    "Note that the training error $E_{\\rm train}(\\alpha)$ is measured on the training data $\\mathbb{X}^{t}$ which was also used to tune the predictor $h^{(\\alpha)}$ (in the above opimtization problem). Therefore, $E_{\\rm train}(\\alpha)$ is too optimistic as a measure for the average error of $h^{(\\alpha)}$ on new data points. Instead, we will measure the quality of $h^{(\\alpha)}$ via the validation error  \n",
    "\\begin{equation} \n",
    "E_{\\rm val}(\\alpha) = (1/m_{v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h^{(\\alpha)}(\\mathbf{x}^{(i)}) \\big)^{2} \n",
    "\\end{equation}\n",
    "incurred by the predictor $h^{(\\alpha)}$ on the validation set $\\mathbb{X}^{(v)}$. We then choose the value $\\alpha$ resulting in the smallest validation error $E_{\\rm val}(\\alpha)$. This grid search can be computationally expensive since we have to solve a separate optimization problem (of minimizing the regularized training error) for each value of $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e4d019532f4bff3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='lassoParameter'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Tuning Lasso Parameter.\n",
    "    \n",
    "Complete the function `lasso_param_search` that computes the Lasso estimator $h^{(\\alpha)}$ for each value $\\alpha$ in the input parameter `alpha_values`. The function returns the resulting validation errors $E_{\\rm val}(\\alpha^{(i)})$ and training errors $E_{\\rm train}(\\alpha^{(i)})$ in the numpy arrays `err_val` of shape (`n_values`,1) and `err_train` of shape (`n_values`,1), as well as the weight vector of the optimal model with the optimal alpha $\\hat{\\alpha}$ (yielding the smallest validation error) in the variable `w_opt`. In the error arrays, the first entry `err_val[0]` should be $E_{\\rm val}(\\alpha^{(1)})$, and so on. \n",
    "\n",
    "[scikit-learn function for Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef618671a3220a4f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lasso_param_search(X_train, X_val, y_train, y_val, alpha_values):\n",
    "    n_values = len(alpha_values)    # The number of candidate values for 'alpha'\n",
    "    err_train = np.zeros([n_values,1])    # Array for training errors\n",
    "    err_val = np.zeros([n_values,1])    # Array for validation errors\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    # Pseudocode:\n",
    "    # -For each alpha in alpha_values:\n",
    "    #   -fit a lasso model on the training data\n",
    "    #   -calculate and store training and validation errors\n",
    "    # -Find the best alpha (i.e. the one with the lowest validation error)\n",
    "    # -Calculate/retrieve the optimal weights (coefficients) corresponding to this alpha\n",
    "    ### BEGIN SOLUTION\n",
    "    for l in range(n_values):\n",
    "        lasso = Lasso(alpha=alpha_values[l], fit_intercept=False)\n",
    "        lasso = lasso.fit(X_train, y_train)\n",
    "        y_train_pred = lasso.predict(X_train)\n",
    "        err_train[l] = mean_squared_error(y_train_pred, y_train)\n",
    "        y_val_pred = lasso.predict(X_val)\n",
    "        err_val[l] = mean_squared_error(y_val_pred, y_val)\n",
    "\n",
    "    best_alpha_idx = np.argmin(err_val)\n",
    "    lasso = Lasso(alpha=alpha_values[best_alpha_idx], fit_intercept=False)\n",
    "    lasso = lasso.fit(X_train, y_train)\n",
    "    w_opt = lasso.coef_\n",
    "    ### END SOLUTION\n",
    "    return w_opt, err_train, err_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b3b01a65b2b8c214",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Specify a list of values for alpha to be considered\n",
    "alpha_values = np.array([0.01, 0.05, 0.2, 1, 3, 10, 1e2, 1e3, 1e4])\n",
    "\n",
    "# Calculate the optimal weights, and training and validation errors for the alpha values defined above using lasso_param_search\n",
    "w_opt, err_train, err_val = lasso_param_search(X_train, X_val, y_train, y_val, alpha_values)\n",
    "\n",
    "# Perform some sanity checks on the outputs\n",
    "assert w_opt.reshape(-1,1).shape == (10,1), \"'w_opts' has wrong shape\"\n",
    "assert len(err_train) == 9, \"'err_train' has wrong shape\"\n",
    "assert len(err_val) == 9, \"'err_val' has wrong shape\"\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "def t_lasso_param_search(X_train, X_val, y_train, y_val, alpha_values):\n",
    "    n_values = len(alpha_values)\n",
    "    err_train = np.zeros([n_values,1]) # Array for training errors\n",
    "    err_val = np.zeros([n_values,1]) # Array for validation errors\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    ### BEGIN SOLUTION\n",
    "    for l in range(n_values):\n",
    "        lasso = Lasso(alpha=alpha_values[l], fit_intercept=False)\n",
    "        lasso = lasso.fit(X_train, y_train)\n",
    "        y_train_pred = lasso.predict(X_train)\n",
    "        err_train[l] = mean_squared_error(y_train_pred, y_train)\n",
    "        y_val_pred = lasso.predict(X_val)\n",
    "        err_val[l] = mean_squared_error(y_val_pred, y_val)\n",
    "\n",
    "    best_alpha_idx = np.argmin(err_val)\n",
    "    lasso = Lasso(alpha=alpha_values[best_alpha_idx], fit_intercept=False)\n",
    "    lasso = lasso.fit(X_train, y_train)\n",
    "    w_opt = lasso.coef_\n",
    "    w_opt = w_opt.reshape(-1,1)\n",
    "    ### END SOLUTION\n",
    "    return w_opt, err_train, err_val\n",
    "    \n",
    "t_w_opt, t_err_train, t_err_val = t_lasso_param_search(X_train, X_val, y_train, y_val, alpha_values)\n",
    "    \n",
    "np.testing.assert_allclose(t_err_val.reshape(-1,1), err_val.reshape(-1,1), rtol=1e-10, atol=0)\n",
    "np.testing.assert_allclose(t_err_train.reshape(-1,1), err_train.reshape(-1,1), rtol=1e-10, atol=0)\n",
    "np.testing.assert_allclose(t_w_opt, w_opt.reshape(-1,1), rtol=1e-10, atol=0)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-535f1b0a4667461c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation errors\n",
    "plt.figure(figsize=(10,6))    # Set figure size\n",
    "plt.plot(alpha_values, err_train, marker='o', color='black', label='training error')    # Plot training errors\n",
    "plt.plot(alpha_values, err_val, marker='o', color='red', label='validation error')    # Plot validation errors\n",
    "plt.xscale('log')    # Set x-axis to logarithmic scale\n",
    "plt.xlabel(r'$\\alpha$')    # Set label of x-axis\n",
    "plt.ylabel(r'$E(\\alpha)$')    # Set label of y-axis\n",
    "plt.title(r'Errors with respect to $\\alpha$', fontsize=16)    # Set title\n",
    "plt.legend()    # Show legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25e8202490edf5af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Take Home Quiz\n",
    "\n",
    "Answer the following questions by setting, for each question, the variable `answer_R4_Q??` to the index of the correct answer. E.g. if you think that the second answer in the first quiz question is the right one, then set `answer_R4_Q1=2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30fe04a19dab009c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR4_1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question R4.1. </p>\n",
    "\n",
    "<p>What is the goal of model selection in machine learning?</p>\n",
    "\n",
    "<ol>\n",
    "  <li> To choose (learn) the optimal predictor function $h_{\\rm opt}$ out of a given hypothesis space (model) $\\mathcal{H}$.</li>\n",
    "  <li> To select the most suitable car model using machine learning methods.</li>\n",
    "  <li> To select the optimal weights used for regularization.</li>\n",
    "  <li> To select the best hypothesis space out of a set of candidates $\\lbrace \\mathcal{H}^{(1)}, \\mathcal{H}^{(2)}, \\ldots,\\mathcal{H}^{(n)} \\rbrace$.</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01dd62184f9f57d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_R4_Q1  = ...\n",
    "### BEGIN SOLUTION\n",
    "answer_R4_Q1 = 4\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b0c15dcdd6f57af7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "assert answer_R4_Q1 in [1,2,3,4], '\"answer_R4_Q1\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert answer_R4_Q1 in [1,2,3,4], '\"answer_R4_Q1\" Value should be an integer between 1 and 4.'\n",
    "assert answer_R4_Q1 == 4, ' \"answer_R4_Q1\" Correct answer is 4.'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d849c30b77c3a9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR4_2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question R4.2. </p>\n",
    "\n",
    "<p>What is a good measure for the prediction error (loss) incurred by a predictor function $h(\\mathbf{x})$ on new data points?</p>\n",
    "<ol>\n",
    "  <li> The empirical error (average loss) of $h(\\mathbf{x})$ on the <b>training set</b> which is also used to tune $h(\\mathbf{x})$. </li>\n",
    "  <li> The empirical error (average loss) of $h(\\mathbf{x})$ on some <b>validation set</b> which is different from the training set. \n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-786b53bfbed054d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_R4_Q2  = ...\n",
    "### BEGIN SOLUTION\n",
    "answer_R4_Q2 = 2\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fcd79fc73255e27d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "assert answer_R4_Q2 in [1,2], '\"answer_R4_Q2\" Value should be an integer between 1 and 2.'\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert answer_R4_Q2 in [1,2], '\"answer_R4_Q2\" Value should be an integer between 1 and 2.'\n",
    "assert answer_R4_Q2 == 2, ' \"answer_R4_Q2\" Correct answer is 2.'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e89debc2a73facf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR4_3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question R4.3. </p>\n",
    "\n",
    "Regularized linear regression amounts to finding the predictor $h(\\mathbf{x})$ which minimizes the regularized training error \n",
    "\\begin{equation} \n",
    "(1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha \\mathcal{R}(h).\n",
    "\\end{equation}\n",
    "Which statement is true?\n",
    "\n",
    "<ol>\n",
    "  <li> Using a large value for the regularization parameter $\\alpha$ prefers predictors with large complexity $\\mathcal{R}(h)$ but small training error.</li>\n",
    "  <li>  Using a small value for the regularization parameter $\\alpha$ prefers predictors with large complexity $\\mathcal{R}(h)$ but small training error.</li>\n",
    "  <li> For regularization parameter $\\alpha=0$, the optimal predictor is always $h(\\mathbf{x}) =0$. </li>\n",
    "  <li> For regularization parameter $\\alpha=0$, the optimal predictor is always $h(\\mathbf{x}) =42$.</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a7f812f9a71b8f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_R4_Q3  = ...\n",
    "### BEGIN SOLUTION\n",
    "answer_R4_Q3 = 2\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-530e6746dd4af06c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "assert answer_R4_Q3 in [1,2,3,4], '\"answer_R4_Q3\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert answer_R4_Q3 in [1,2,3,4], '\"answer_R4_Q3\" Value should be an integer between 1 and 4.'\n",
    "assert answer_R4_Q3 == 2, ' \"answer_R4_Q3\" Correct answer is 2.'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bd3fa9a8a8b2da9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR4_4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question R4.4. </p>\n",
    "\n",
    "Use the previously implemented code in \"Tuning Lasso Parameter\" to find the optimal predictor (lowest validation error) for $\\alpha=$ `alpha_val`. Using the same dataset.\n",
    "When initializing Lasso, please use `fit_intercept=False`. \n",
    "<p>Which alpha should be chosen to achieve an optimal predictor?</p> \n",
    "<p>Possible alpha values are given in the code cell below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11392734fc803cfe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_values = np.array([0.01, 0.05, 0.2, 1, 3, 10, 1e2, 1e3])\n",
    "\n",
    "# answer_R4_Q4  = ...\n",
    "### BEGIN SOLUTION\n",
    "answer_R4_Q4 = 1e3\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d7f48b684296863d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "assert answer_R4_Q4 in [0.01, 0.05, 0.2, 1, 3, 10, 1e2, 1e3], 'answer_R4_Q3\" Value should be a value out of given alpha values..'\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert answer_R4_Q4 == 1e3, ' answer_R4_Q4\" Correct answer is 1e3.'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
